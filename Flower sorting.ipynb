{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d1c3131-8135-4496-a635-30913591c0de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3670 files belonging to 5 classes.\n",
      "Using 2936 files for training.\n",
      "Found 3670 files belonging to 5 classes.\n",
      "Using 734 files for validation.\n",
      "Epoch 1/13\n",
      "92/92 [==============================] - 7s 69ms/step - loss: 1.4890 - accuracy: 0.3743 - val_loss: 1.1502 - val_accuracy: 0.5068\n",
      "Epoch 2/13\n",
      "92/92 [==============================] - 6s 61ms/step - loss: 1.0984 - accuracy: 0.5593 - val_loss: 1.0856 - val_accuracy: 0.5749\n",
      "Epoch 3/13\n",
      "92/92 [==============================] - 6s 64ms/step - loss: 1.0195 - accuracy: 0.6121 - val_loss: 1.1046 - val_accuracy: 0.6076\n",
      "Epoch 4/13\n",
      "92/92 [==============================] - 6s 65ms/step - loss: 1.0528 - accuracy: 0.6410 - val_loss: 1.4436 - val_accuracy: 0.5272\n",
      "Epoch 5/13\n",
      "92/92 [==============================] - 6s 62ms/step - loss: 0.9278 - accuracy: 0.7047 - val_loss: 1.9814 - val_accuracy: 0.5395\n",
      "Epoch 6/13\n",
      "92/92 [==============================] - 6s 62ms/step - loss: 3.1545 - accuracy: 0.5760 - val_loss: 2.6349 - val_accuracy: 0.5082\n",
      "Epoch 7/13\n",
      "92/92 [==============================] - 6s 59ms/step - loss: 3.0471 - accuracy: 0.5531 - val_loss: 3.4189 - val_accuracy: 0.4496\n",
      "Epoch 8/13\n",
      "92/92 [==============================] - 6s 60ms/step - loss: 1.7141 - accuracy: 0.6158 - val_loss: 2.2023 - val_accuracy: 0.5395\n",
      "Epoch 9/13\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 1.3379 - accuracy: 0.7010 - val_loss: 3.0934 - val_accuracy: 0.5109\n",
      "Epoch 10/13\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 4.6605 - accuracy: 0.6291 - val_loss: 11.4925 - val_accuracy: 0.4877\n",
      "Epoch 11/13\n",
      "92/92 [==============================] - 5s 58ms/step - loss: 13.5755 - accuracy: 0.6107 - val_loss: 25.8982 - val_accuracy: 0.4837\n",
      "Epoch 12/13\n",
      "92/92 [==============================] - 5s 56ms/step - loss: 57.2165 - accuracy: 0.5221 - val_loss: 79.4486 - val_accuracy: 0.4455\n",
      "Epoch 13/13\n",
      "92/92 [==============================] - 5s 59ms/step - loss: 94.9600 - accuracy: 0.5848 - val_loss: 209.5645 - val_accuracy: 0.4455\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pathlib\n",
    "\n",
    "dataset_url = \"https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz\"\n",
    "data_dir = tf.keras.utils.get_file('flower_photos.tar', origin=dataset_url, extract=True)\n",
    "data_dir = pathlib.Path(data_dir).with_suffix('')\n",
    "#import image set correctly\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "img_height, img_width = 180, 180\n",
    "#batch size splits dataset, each image is 180x180x3 - 3 for RGB\n",
    "\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(data_dir, validation_split=0.2, subset=\"training\",\n",
    "  seed=123, image_size=(img_height, img_width),batch_size=batch_size)\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(data_dir, validation_split=0.2, subset=\"validation\",\n",
    "  seed=123, image_size=(img_height, img_width),batch_size=batch_size)\n",
    "#split data into training set(0.8) and validation set(0.2)\n",
    "\n",
    "norm_layer = tf.keras.layers.Rescaling(1./255)\n",
    "#preprocessing layer to turn [0,255] into [0,1]\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "#to allow for efficient I/O to GPU\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255, input_shape = (img_height, img_width, 3)),\n",
    "  tf.keras.layers.Conv2D(16, 3, padding = 'same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, padding = 'same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(64, 3, padding = 'same', activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])\n",
    "# Sequential means passes linearly through. First detect neighbourhood patterns in increasing detail wiht conv,\n",
    "# then flatten to 1d and use dense to connect to find global features. Output with 5 classes - flower types\n",
    "\n",
    "model.compile(optimizer = 'adam',loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics = ['accuracy'])\n",
    "#define optimization program (i.e. gradient descent), loss function and metrics is for reviewing training later\n",
    "\n",
    "epochs = 13\n",
    "history = model.fit(train_ds, validation_data = val_ds, epochs = epochs)\n",
    "#trains the model, fits training data to validation data and repeats 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "29205491-7bab-45a4-8052-4d9748474799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://static.scientificamerican.com/sciam/cache/file/A96AF6D1-484D-412F-8E43F6D8882F32AA_source.jpg\n",
      "388821/388821 [==============================] - 0s 0us/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "daisy: 0.00\n",
      "dandelion: 0.00\n",
      "roses: 1.00\n",
      "sunflowers: 0.00\n",
      "tulips: 0.00\n",
      "This image most likely belongs to roses with a 100.00 percent confidence.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "url = 'https://static.scientificamerican.com/sciam/cache/file/A96AF6D1-484D-412F-8E43F6D8882F32AA_source.jpg'\n",
    "image = tf.keras.utils.load_img(tf.keras.utils.get_file('/Users/joeladams/Documents/Python/Machine Learning/temp.jpg', url), target_size = (180,180))\n",
    "image.show()\n",
    "\n",
    "img_array = tf.keras.utils.img_to_array(image)\n",
    "img_array = tf.expand_dims(img_array, 0) # Create a batch\n",
    "\n",
    "predictions = model.predict(img_array)\n",
    "score = tf.nn.softmax(predictions[0])\n",
    "\n",
    "for i in range(5):\n",
    "    print('{}: {:.2f}'.format(class_names[i], score[i]))\n",
    "\n",
    "print(\n",
    "    \"This image most likely belongs to {} with a {:.2f} percent confidence.\"\n",
    "    .format(class_names[np.argmax(score)], 100 * np.max(score))\n",
    ")\n",
    "os.remove('/Users/joeladams/Documents/Python/Machine Learning/temp.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b230fb-2693-45e1-ad0c-fcf637afa2b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb86cdd-e710-4363-ac04-e70186cd9f16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
